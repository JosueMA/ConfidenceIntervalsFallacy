<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="favicon.ico">

    <title>The Fallacy of Placing Confidence in Confidence Intervals</title>

   <!-- Markdown CSS -->
   <link href="css/markdown.css" rel="stylesheet">


    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/sticky-footer-navbar.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- hypothes.is -->
    <script async defer src="//hypothes.is/embed.js"></script>
  
    <!-- lightbox style -->
    <link href="css/slimbox2.css" rel="stylesheet">

    <!-- my article style -->
    <link href="css/article.css" rel="stylesheet">

    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: 'none',
        // show equation numbers
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        'HTML-CSS': {
          imageFont: null
        }
      });
    </script>


  </head>

  <body>

    <!-- Fixed navbar -->
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">The Fallacy of Placing Confidence in Confidence Intervals</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">Home</a></li>
            <li class="dropdown active">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Main paper<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li class=""><a href="introduction.html">Introduction</a></li>
                <li><a href="folktheory.html">The folk theory of confidence intervals</a></li>
                <li><a href="CItheory.html">The theory of confidence intervals</a></li>
                <li role="separator" class="divider"></li>
                <li><a href="lostsub.html">Example 1: The Lost Submarine</a></li>
                <li><a href="subCIs.html">Five confidence procedures</a></li>
                <li><a href="subCIproperties.html">Properties of the procedures</a></li>                
                <li><a href="subCIevaluation.html">Evaluating the procedures</a></li>
                <li role="separator" class="divider"></li>
                <li class="active"><a href="omegasqCI.html">Example 2: A confidence interval in the wild</a></li>
                <li role="separator" class="divider"></li>
                <li><a href="discussion.html">Discussion</a></li>
                <li><a href="guidelines.html">Guidelines for interpreting and reporting intervals</a></li>
                <li><a href="confvscred.html">Confidence intervals versus credible intervals</a></li>
                <li><a href="conclusion.html">Conclusion</a></li>
                <li role="separator" class="divider"></li>
                <li><a href="references.html">References</a></li>
              </ul>
            </li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Supplements<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="supplement.html">Details and code</a></li>
                <li><a href="discussion_guide.html">Discussion guide</a></li>
              </ul>
            </li>            
            <li><a href="contact.html">Contact</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <!-- Begin page content -->
    <div class="container">
      <button onclick="TogetherJS(this); return false;">Start collaboration with TogetherJS</button>
      <p><a href="https://twitter.com/share" class="twitter-share-button" data-via="richarddmorey" data-hashtags="statistics">Tweet</a>
          <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');
          </script>
        </p>

      <div class="page-header">
        <h2>Example 2: A confidence interval in the wild</h2>
      </div>
      
      <p>The previous example was designed to show, in an accessible example, the logic of confidence interval theory. Further, it showed that confidence procedures cannot be assumed to have the properties that analysts desire.</p>
      <p>When presenting the confidence intervals, CI proponents almost always focus on estimation of the mean of a normal distribution. In this simple case, frequentist and Bayesian (with a “non-informative” prior) answers numerically coincide.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> However, the proponents of confidence intervals suggest the use of confidence intervals for many other quantities: for instance, standardized effect size Cohen’s <span class="math">\(d\)</span> <span class="citation">(G. Cumming &amp; Finch, 2001)</span>, medians <span class="citation">(Bonett &amp; Price, 2002; Olive, 2008)</span>, correlations <span class="citation">(Zou, 2007)</span>, ordinal association <span class="citation">(Woods, 2007)</span>, and many others. Quite often authors of such articles provide no analysis of the properties of the proposed confidence procedures beyond showing that they contain the true value in the correct proportion of samples: that is, that they are confidence procedures. Sometimes the authors provide an analysis of the frequentist properties of the procedures, such as average width. The developers of new confidence procedures do not, as a rule, examine whether their procedures allow for valid post-data reasoning.</p>
      <p>As the first example showed, a sole focus on frequentist properties of procedures is potentially disastrous for users of these confidence procedures because a confidence procedure has no guarantee of supporting reasonable inferences about the parameter of interest. <span class="citation">Casella (1992)</span> underscores this point with confidence intervals, saying that “we must remember that practitioners are going to make conditional (post-data) inferences. Thus, we must be able to assure the user that any inference made, either pre-data or post-data, possesses some definite measure of validity” (p. 10). Any development of an interval procedure that does not, at least in part, focus on its post-data properties is incomplete at best and extremely misleading at worst: <em>caveat emptor</em>.</p>
      <p>Can such misleading inferences occur using procedures suggested by proponents of confidence intervals, and in use by researchers? The answer is yes, which we will show by examining a confidence interval for <span class="math">\(\omega^2\)</span>, the proportion of variance accounted for in ANOVA designs. The parameter <span class="math">\(\omega^2\)</span> serves as a measure of effect size when there are more than two levels in a one-way design. This interval was suggested by <span class="citation">(Steiger, 2004;see also Steiger &amp; Fouladi, 1997)</span>, cited approvingly by <span class="citation">G. Cumming (2014)</span>, implemented in software for social scientists <span class="citation">(e.g., Kelley, 2007a, 2007b)</span>, and evaluated, solely for its frequentist properties, by <span class="citation">W. H. Finch &amp; French (2012)</span>. The problems we discuss here are shared by other related confidence intervals, such as confidence intervals for <span class="math">\(\eta^2\)</span>, partial <span class="math">\(\eta^2\)</span>, the noncentrality parameter of the <span class="math">\(F\)</span> distribution, the signal-to-noise ratio <span class="math">\(f\)</span>, RMSSE <span class="math">\(\Psi\)</span>, and others discussed by Steiger (2004).</p>
      <p><span class="citation">Steiger (2004)</span> introduces confidence intervals by emphasizing a desire to avoid significance tests, and to focus more on the precision of estimates. Steiger says that “the scientist is more interested in knowing how large the difference between the two groups is (and how precisely it has been determined) than whether the difference between the groups is 0” (pp. 164-165). Steiger and Fouladi (1997) say that “[t]he advantage of a confidence interval is that the width of the interval provides a ready indication of the precision of measurement…” (p. 231). Given our knowledge of the precision fallacy these statements should raise a red flag.</p>
      <p>Steiger then offers a confidence procedure for <span class="math">\(\omega^2\)</span> by inverting a significance test. Given the strange behavior of the UMP procedure in the submersible example, this too should raise a red flag. A confidence procedure based on a test — even a good, high-powered test — will not in general yield a procedure that provides for reasonable inferences. We will outline the logic of building a confidence interval by inverting a significance test before showing how Steiger’s confidence interval behaves with data.</p>

      <div class="article_figure">
              <a href="figures/CIinvert1-1.svg" rel="lightbox" title="Figure 6: Building a confidence interval by inverting a significance test. A: Two noncentral F distributions, with true ω&lt;sup&gt;2&lt;/sup&gt;=.1 (blue solid line) and true ω&lt;sup&gt;2&lt;/sup&gt;=.2 (dashed gray line). When F(2,27)=5, the upper-tailed p value for these tests are .16 and .42, respectively. B: Two noncentral F distributions, with true ω&lt;sup&gt;2&lt;/sup&gt;=.36 (red solid line) and true ω&lt;sup&gt;2&lt;/sup&gt;=.2 (dashed gray line). When F(2,27)=5, the lower-tailed p value for these tests are .16 and .58, respectively." name="CIinvert1"></a>
      </div>

      <p>To understand how a confidence interval can be built by inverting a significance test, consider that a two-sided significance test of size <span class="math">\(\alpha\)</span> can be thought of as a combination of two one-sided tests at size <span class="math">\(\alpha/2\)</span>: one for each tail. The two-sided test rejects when one of the one-tailed tests rejects. To build a 68% confidence interval (i.e., an interval that covers the true value as often as the commonly-used standard error for the normal mean), we can use two one-sided tests of size <span class="math">\((1 - .68)/2 = .16\)</span>. Suppose we have a one-way design with three groups and <span class="math">\(N=10\)</span> participants in each group. The effect size <span class="math">\(\omega^2\)</span> in such a design indexes how large <span class="math">\(F\)</span> will be: larger <span class="math">\(\omega^2\)</span> values tend to yield larger <span class="math">\(F\)</span> values. The distribution of <span class="math">\(F\)</span> given the effect size <span class="math">\(\omega^2\)</span> is called the noncentral <span class="math">\(F\)</span> distribution. When <span class="math">\(\omega^2=0\)</span> — that is, there is no effect — the familiar central <span class="math">\(F\)</span> distribution is obtained.</p>
      <p>Consider first a one-sided test that rejects when <span class="math">\(F\)</span> is large. <a href="#CIinvert1">Figure 6</a>A shows that a test of the null hypothesis that <span class="math">\(\omega^2=.1\)</span> would yield <span class="math">\(p=.16\)</span> when <span class="math">\(F(2,27)=5\)</span>. If we tested larger values of <span class="math">\(\omega^2\)</span>, the <span class="math">\(F\)</span> value would not lead to a rejection; if we tested smaller values of <span class="math">\(\omega^2\)</span>, they would be rejected because their <span class="math">\(p\)</span> values would be below <span class="math">\(.16\)</span>. The gray dashed line in <a href="#CIinvert1">Figure 6</a>A shows the noncentral <span class="math">\(F(2,27)\)</span> distribution for <span class="math">\(\omega^2=.2\)</span>; it is apparent that the <span class="math">\(p\)</span> value for this test would be greater than .16, and hence <span class="math">\(\omega^2=.2\)</span> would not be rejected by the upper-tailed test of size .16. Now consider the one-sided test that rejects when <span class="math">\(F\)</span> is small. <a href="#CIinvert1">Figure 6</a>B shows that a test of the null hypothesis that <span class="math">\(\omega^2=.36\)</span> would yield <span class="math">\(p=.16\)</span> when <span class="math">\(F(2,27)=5\)</span>; any <span class="math">\(\omega^2\)</span> value greater than .36 would be rejected with <span class="math">\(p&lt;.16\)</span>, and any <span class="math">\(\omega^2\)</span> value less than .36 would not.</p>
      <p>Considering the two one-tailed tests together, for any <span class="math">\(\omega^2\)</span> value in <span class="math">\([.1, .36]\)</span>, the <span class="math">\(p\)</span> value for both one-sided tests will be greater than <span class="math">\(p&gt;.16\)</span> and hence will not lead to a rejection. A 68% confidence interval for when <span class="math">\(F(2,27)=5\)</span> can be defined as all <span class="math">\(\omega^2\)</span> values that are not rejected by either of the two-tailed tests, and so <span class="math">\([.1,.36]\)</span> is taken as a 68% confidence interval. A complication arises, however, when the <span class="math">\(p\)</span> value from the ANOVA <span class="math">\(F\)</span> test is greater than <span class="math">\(\alpha/2\)</span>; by definition, the <span class="math">\(p\)</span> value is computed under the hypothesis that there is no effect, that is <span class="math">\(\omega^2=0\)</span>. Values of <span class="math">\(\omega^2\)</span> cannot be any lower than 0, and hence there are no <span class="math">\(\omega^2\)</span> values that would be rejected by the upper tailed test. In this case the lower bound on the CI does not exist. A second complication arises when the <span class="math">\(p\)</span> value is greater than <span class="math">\(1-\alpha/2\)</span>: all lower-tailed tests will reject, and hence the upper bound of the CI does not exist. If a bound does not exist, Steiger (2004) arbitrarily sets it at 0.</p>
      <p>To see how this CI works in practice, suppose we design a three-group, between-subjects experiment with <span class="math">\(N=10\)</span> participants in each group and obtain an <span class="math">\(F(2,27)=0.18, p = 0.84\)</span>. Following recommendations for good analysis practices <span class="citation">(e.g., Psychonomics Society, 2012; Wilkinson &amp; Task Force on Statistical Inference, 1999)</span>, we would like to compute a confidence interval on the standardized effects size <span class="math">\(\omega^2\)</span>. Using software to compute Steiger’s CI, we obtain the 68% confidence interval <span class="math">\([0,0.01]\)</span>.</p>

      <div class="article_figure">
              <a href="figures/steiger1-1.svg" rel="lightbox" title="Figure 7: Likelihoods, confidence intervals, and Bayesian credible intervals (highest posterior density, or HPD, intervals) for four hypothetical experimental results. In each figure, the top interval is Steiger’s (2004) confidence interval for ω&lt;sup&gt;2&lt;/sup&gt;; the bottom interval is the Bayesian HPD. See text for details." name="steiger1"></a>
      </div>

      <p><a href="#steiger1">Figure 7</a>A (top interval) shows the resulting 68% interval. If we were not aware of the fallacies of confidence intervals, we might publish this confidence interval thinking it provides a good measure of the precision of the estimate of <span class="math">\(\omega^2\)</span>. Note that the lower limit of the confidence interval is exactly 0, because the lower bound did not exist. In discussing this situation <span class="citation">Steiger &amp; Fouladi (1997)</span>] say</p>
      <blockquote>
        <p>“[Arbitrarily setting the confidence limit at 0] maintains the correct coverage probability for the confidence interval, but the width of the confidence interval may be suspect as an index of the precision of measurement when either or both ends of the confidence interval are at 0. In such cases, one might consider obtaining alternative indications of precision of measurement, such as an estimate of the standard error of the statistic.” (Steiger and Fouladi, 1997, p. 255)</p>
      </blockquote>
      <p><span class="citation">Steiger (2004)</span> further notes that “relationship [between CI width and precision] is less than perfect and is seriously compromised in some situations for several reasons” (p. 177). This is a rather startling admission: a major part of the justification for confidence intervals, including the one computed here, is that confidence intervals supposedly allow an assessment of the precision with which the parameter is estimated. The confidence interval fails to meet the purpose for which it was advocated in the first place, but Steiger does not indicate why, nor under what conditions the CI will successfully track precision.</p>
      <p>We can confirm the need for Steiger’s caution — essentially, a warning about the precision fallacy — by looking at the likelihood, which is the probability density of the observed <span class="math">\(F\)</span> statistic computed for all possible true values of <span class="math">\(\omega^2\)</span>. Notice how narrow the confidence interval is compared to the likelihood of <span class="math">\(\omega^2\)</span>. The likelihood falls much more slowly as <span class="math">\(\omega^2\)</span> gets larger than the confidence interval would appear to imply, if we believed the precision fallacy. We can also compare the confidence interval to a 68% Bayesian credible interval, computed assuming standard “noninformative” priors on the means and the error variance.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> The Bayesian credible interval is substantially wider, revealing the imprecision with which <span class="math">\(\omega^2\)</span> is estimated.</p>
      <p><a href="#steiger1">Figure 7</a>B shows the same case, but for a slightly smaller <span class="math">\(F\)</span> value. The precision with which <span class="math">\(\omega^2\)</span> is estimated has not changed to any substantial degree; yet now the confidence interval contains only the value <span class="math">\(\omega^2=0\)</span>: or, more accurately, the confidence interval is empty because this <span class="math">\(F\)</span> value would always be rejected by one of the pairs of one-sided tests that led to the construction of the confidence interval. As Steiger points out, a “zero-width confidence interval obviously does not imply that effect size was determined with perfect precision,” (p. 177), nor can it imply that there is a 68% probability that <span class="math">\(\omega^2\)</span> is exactly 0. This can be clearly seen by examining the likelihood and Bayesian credible interval.</p>
      <p>Some authors <span class="citation">(e.g. Dufour, 1997)</span> interpret empty confidence intervals as indicative of model misfit. In the case of this one sample design, if the confidence interval is empty then the means are more similar than would be expected even under the null hypothesis <span class="math">\(\alpha/2\)</span> of the time; that is, <span class="math">\(p&gt;1-\alpha/2\)</span>, and hence <span class="math">\(F\)</span> is small. If this model rejection significance test logic is used, the confidence interval itself becomes uninterpretable as the model gets close to rejection, because it appears to indicate false precision <span class="citation">(Gelman, 2011)</span>. Moreover, in this case the <span class="math">\(p\)</span> value is certainly more informative than the CI; the <span class="math">\(p\)</span> value provides graded information that does not depend on the arbitrary choice of <span class="math">\(\alpha\)</span>, while the CI is simply empty for all values of <span class="math">\(p&gt;1-\alpha/2\)</span>.</p>
      <p><a href="#steiger1">Figure 7</a>C shows what happens when we increase the confidence coefficient slightly to 70%. Again, the precision with which the parameter is estimated has not changed, yet the confidence interval now again has nonzero width.</p>
      <p><a href="#steiger1">Figure 7</a>D shows the results of an analysis with <span class="math">\(F(2,27)=4.24, p = 0.03\)</span>, and using a 95% confidence interval. Steiger’s interval has now encompassed most of the likelihood, but the lower bound is still “stuck” at 0. In this situation, Steiger and Fouladi advise us that the width of the CI is “suspect” as an indication of precision, and that we should “obtain[] [an] alternative indication[] of precision of measurement.” As it turns out, here the confidence interval is not too different from the credible interval, though the confidence interval is longer and is unbalanced. However, we would not know this if we did not examine the likelihood and the Bayesian credible interval; the only reason we know the confidence interval has a reasonable width in this particular case is its agreement with the actual measures of precision offered by the likelihood and the credible interval.</p>
      <p>How often will Steiger’s confidence procedure yield a “suspect” confidence interval? This will occur whenever the <span class="math">\(p\)</span> value for the corresponding <span class="math">\(F\)</span> test is <span class="math">\(p&gt;\alpha/2\)</span>; for a 95% confidence interval, this means that whenever <span class="math">\(p&gt;0.025\)</span>, Steiger and Fouladi recommend against using the confidence interval for precisely the purpose that they — and other proponents of confidence intervals — recommend it for. This is not a mere theoretical issue; moderately-sized <span class="math">\(p\)</span> values often occur. In a cursory review of papers citing <span class="citation">Steiger (2004)</span>, we found many that obtained and reported, without note, suspect confidence intervals bounded at 0 <span class="citation">(e.g., S. P. Cumming, Sherar, Gammon, Standage, &amp; Malina, 2012; Gilroy &amp; Pearce, 2014; Hamerman &amp; Morewedge, 2015, 2015; Lahiri, Maloney, Rogers, &amp; Ge, 2013; Todd, Vurbic, &amp; Bouton, 2014; Winter et al., 2014)</span>. The others did not use confidence intervals, instead relying on point estimates of effect size and <span class="math">\(p\)</span> values <span class="citation">(e.g., Hollingdale &amp; Greitemeyer, 2014)</span>; but from the <span class="math">\(p\)</span> values it could be inferred that if they had followed “good practice” and computed such confidence intervals, they would have obtained intervals that according to Steiger could not be interpreted as anything but an inverted <span class="math">\(F\)</span> test.</p>
      <p>It makes sense, however, that authors using confidence intervals would not note that the interpretation of their confidence intervals is problematic. If confidence intervals truly contained the most likely values, or if they were indices of the precision, or if the confidence coefficient indexed the uncertainty we should have that the parameter is in an interval, then it would seem that a CI is a CI: what you learn from one is the same as what you learn from another. The idea that the <span class="math">\(p\)</span> value can determine whether the interpretation of a confidence interval is possible is not intuitive in light of the way CIs are typically presented.</p>
      <p>We see no reason why our ability to interpret an interval <em>should</em> be compromised simply because we obtained a <span class="math">\(p\)</span> value that was not low enough. Certainly, the confidence coefficient is arbitrary; if the width is suspect for one confidence coefficient, it makes little sense that the CI width would become acceptable just because we changed the confidence coefficient so the interval bounds did not include 0. Also, if the width is too narrow with moderate <span class="math">\(p\)</span> values, such that it is not an index of precision, it seems that the interval will be too wide in other circumstances, possibly threatening the interpretation as well. This was evident with the UMP procedure in the submersible example: the UMP interval was too narrow when the data provided little information, and was too wide when the data provided substantial information.</p>
      <p>Steiger and Fouladi (1997) summarize the central problem with confidence intervals when they say that in order to maintain the correct coverage probability — a frequentist pre-data concern — they sacrifice the very thing researchers want confidence intervals to be: a post-data index of the precision of measurement. If our goal is to move away from significance testing, we should not use methods which cannot be interpreted except as inversions of significance tests. We agree with Steiger and Fouladi that researchers should consider obtaining alternative indications of precision of measurement; luckily, Bayesian credible intervals fit the bill rather nicely, rendering confidence intervals unnecessary.</p>

      <div class="footnotes">
        <hr />
        <ol start=6>
          <li id="fn6"><p>This should not be taken to mean that inference by confidence intervals is not problematic even in this simple case; see e.g., <span class="citation">Brown (1967)</span> and <span class="citation">Buehler &amp; Feddersen (1963)</span>.<a href="#fnref6">↩</a></p></li>
          <li id="fn7"><p>See the supplement for details. We do not generally advocate non-informative priors on parameters of interest <span class="citation">(Rouder, Morey, Speckman, &amp; Province, 2012; Wetzels, Grasman, &amp; Wagenmakers, 2012)</span>; in this instance we use them as a comparison because many people believe, incorrectly, that confidence intervals numerically correspond to Bayesian credible intervals with noninformative priors.<a href="#fnref7">↩</a></p></li>
        </ol>
      </div>



      <ul class="pager">
        <li class="previous"><a href="subCIevaluation.html">Previous</a></li>
        <li class="next"><a href="discussion.html">Next</a></li>
      </ul>
    
    </div>


      <footer class="footer">
        <div class="container">
        <p class="text-muted">Please cite as Morey, Hoekstra, Rouder, Lee and Wagenmakers (in press). Psychonomic Bulletin &amp; Review.</p>
        </div>
      </footer>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="js/ie10-viewport-bug-workaround.js"></script>
    <script type="text/javascript" src="js/slimbox2.js"></script>
    <script type="text/javascript" src="js/article.js"></script>
    <script src="https://togetherjs.com/togetherjs-min.js"></script>

    <script>make_figures();</script>
  </body>
</html>
